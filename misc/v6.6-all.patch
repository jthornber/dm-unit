diff --git a/Makefile b/Makefile
index 5c418efbe89b..7cb99423000e 100644
--- a/Makefile
+++ b/Makefile
@@ -895,6 +895,13 @@ ifdef CONFIG_ZERO_CALL_USED_REGS
 KBUILD_CFLAGS	+= -fzero-call-used-regs=used-gpr
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once) \
+		   -fno-ipa-cp -fno-ipa-sra -fno-tree-dce
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_USE_CC
   CC_FLAGS_FTRACE	+= -mrecord-mcount
diff --git a/arch/riscv/Kconfig b/arch/riscv/Kconfig
index 9c48fecc6719..187d5e0b9e79 100644
--- a/arch/riscv/Kconfig
+++ b/arch/riscv/Kconfig
@@ -807,11 +807,8 @@ config EFI
 	  allow the kernel to be booted as an EFI application. This
 	  is only useful on systems that have UEFI firmware.
 
-config CC_HAVE_STACKPROTECTOR_TLS
-	def_bool $(cc-option,-mstack-protector-guard=tls -mstack-protector-guard-reg=tp -mstack-protector-guard-offset=0)
-
 config STACKPROTECTOR_PER_TASK
-	def_bool y
+	def_bool n
 	depends on !RANDSTRUCT
 	depends on STACKPROTECTOR && CC_HAVE_STACKPROTECTOR_TLS
 
diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c
index bc309e41d074..534e64e0f852 100644
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@ -114,7 +114,7 @@ static void lru_destroy(struct lru *lru)
 /*
  * Insert a new entry into the lru.
  */
-static void lru_insert(struct lru *lru, struct lru_entry *le)
+static noinline void lru_insert(struct lru *lru, struct lru_entry *le)
 {
 	/*
 	 * Don't be tempted to set to 1, makes the lru aspect
@@ -235,7 +235,7 @@ static void lru_remove(struct lru *lru, struct lru_entry *le)
 /*
  * Mark as referenced.
  */
-static inline void lru_reference(struct lru_entry *le)
+static noinline void lru_reference(struct lru_entry *le)
 {
 	atomic_set(&le->referenced, 1);
 }
@@ -254,7 +254,7 @@ enum evict_result {
 
 typedef enum evict_result (*le_predicate)(struct lru_entry *le, void *context);
 
-static struct lru_entry *lru_evict(struct lru *lru, le_predicate pred, void *context)
+struct lru_entry *lru_evict(struct lru *lru, le_predicate pred, void *context)
 {
 	unsigned long tested = 0;
 	struct list_head *h = lru->cursor;
@@ -517,7 +517,7 @@ static void cache_init(struct dm_buffer_cache *bc, unsigned int num_locks)
 	lru_init(&bc->lru[LIST_DIRTY]);
 }
 
-static void cache_destroy(struct dm_buffer_cache *bc)
+static noinline void cache_destroy(struct dm_buffer_cache *bc)
 {
 	unsigned int i;
 
@@ -811,7 +811,7 @@ static bool __cache_insert(struct rb_root *root, struct dm_buffer *b)
 	return true;
 }
 
-static bool cache_insert(struct dm_buffer_cache *bc, struct dm_buffer *b)
+static noinline bool cache_insert(struct dm_buffer_cache *bc, struct dm_buffer *b)
 {
 	bool r;
 
diff --git a/drivers/md/dm-cache-metadata.c b/drivers/md/dm-cache-metadata.c
index acffed750e3e..08bd99232c37 100644
--- a/drivers/md/dm-cache-metadata.c
+++ b/drivers/md/dm-cache-metadata.c
@@ -1324,11 +1324,11 @@ static bool hints_array_initialized(struct dm_cache_metadata *cmd)
 	return cmd->hint_root && cmd->policy_hint_size;
 }
 
-static bool hints_array_available(struct dm_cache_metadata *cmd,
+static noinline bool hints_array_available(struct dm_cache_metadata *cmd,
 				  struct dm_cache_policy *policy)
 {
 	return cmd->clean_when_opened && policy_unchanged(cmd, policy) &&
-		hints_array_initialized(cmd);
+	       hints_array_initialized(cmd);
 }
 
 static int __load_mapping_v1(struct dm_cache_metadata *cmd,
diff --git a/drivers/md/dm-thin-metadata.c b/drivers/md/dm-thin-metadata.c
index 6022189c1388..880ad9a8fd89 100644
--- a/drivers/md/dm-thin-metadata.c
+++ b/drivers/md/dm-thin-metadata.c
@@ -7,6 +7,7 @@
 
 #include "dm-thin-metadata.h"
 #include "persistent-data/dm-btree.h"
+#include "persistent-data/dm-extent-allocator.h"
 #include "persistent-data/dm-space-map.h"
 #include "persistent-data/dm-space-map-disk.h"
 #include "persistent-data/dm-transaction-manager.h"
@@ -152,6 +153,7 @@ struct dm_pool_metadata {
 	struct dm_block_manager *bm;
 	struct dm_space_map *metadata_sm;
 	struct dm_space_map *data_sm;
+	struct dm_extent_allocator *data_extents;
 	struct dm_transaction_manager *tm;
 	struct dm_transaction_manager *nb_tm;
 
@@ -232,6 +234,7 @@ struct dm_thin_device {
 	struct list_head list;
 	struct dm_pool_metadata *pmd;
 	dm_thin_id id;
+	struct dm_alloc_context data_alloc;
 
 	int open_count;
 	bool changed:1;
@@ -846,6 +849,12 @@ static int __begin_transaction(struct dm_pool_metadata *pmd)
 	return 0;
 }
 
+static void __free_device(struct dm_thin_device *td)
+{
+	dm_alloc_context_put(&td->data_alloc);
+	kfree(td);
+}
+
 static int __write_changed_details(struct dm_pool_metadata *pmd)
 {
 	int r;
@@ -874,7 +883,7 @@ static int __write_changed_details(struct dm_pool_metadata *pmd)
 			td->changed = false;
 		else {
 			list_del(&td->list);
-			kfree(td);
+			__free_device(td);
 		}
 	}
 
@@ -956,6 +965,7 @@ struct dm_pool_metadata *dm_pool_metadata_open(struct block_device *bdev,
 {
 	int r;
 	struct dm_pool_metadata *pmd;
+	uint64_t nr_blocks;
 
 	pmd = kmalloc(sizeof(*pmd), GFP_KERNEL);
 	if (!pmd) {
@@ -972,6 +982,7 @@ struct dm_pool_metadata *dm_pool_metadata_open(struct block_device *bdev,
 	pmd->data_block_size = data_block_size;
 	pmd->pre_commit_fn = NULL;
 	pmd->pre_commit_context = NULL;
+	pmd->data_extents = NULL;
 
 	r = __create_persistent_data_objects(pmd, format_device);
 	if (r) {
@@ -981,14 +992,31 @@ struct dm_pool_metadata *dm_pool_metadata_open(struct block_device *bdev,
 
 	r = __begin_transaction(pmd);
 	if (r < 0) {
-		if (dm_pool_metadata_close(pmd) < 0)
-			DMWARN("%s: dm_pool_metadata_close() failed.", __func__);
-		return ERR_PTR(r);
+		DMERR("could not begin transaction");
+		goto bad;
 	}
 
 	__set_metadata_reserve(pmd);
 
+	r = dm_sm_get_nr_blocks(pmd->data_sm, &nr_blocks);
+	if (r) {
+		DMERR("could not get size of data device");
+		goto bad;
+	}
+
+	pmd->data_extents = dm_extent_allocator_create(nr_blocks);
+	if (!pmd->data_extents) {
+		DMWARN("could not create data extent allocator");
+		r = -ENOMEM;
+		goto bad;
+	}
+
 	return pmd;
+
+bad:
+	if (dm_pool_metadata_close(pmd) < 0)
+		DMWARN("%s: dm_pool_metadata_close() failed.", __func__);
+	return ERR_PTR(r);
 }
 
 int dm_pool_metadata_close(struct dm_pool_metadata *pmd)
@@ -1003,7 +1031,7 @@ int dm_pool_metadata_close(struct dm_pool_metadata *pmd)
 			open_devices++;
 		else {
 			list_del(&td->list);
-			kfree(td);
+			__free_device(td);
 		}
 	}
 	up_read(&pmd->root_lock);
@@ -1024,6 +1052,9 @@ int dm_pool_metadata_close(struct dm_pool_metadata *pmd)
 	pmd_write_unlock(pmd);
 	__destroy_persistent_data_objects(pmd, true);
 
+	if (pmd->data_extents)
+		dm_extent_allocator_destroy(pmd->data_extents);
+
 	kfree(pmd);
 	return 0;
 }
@@ -1091,6 +1122,7 @@ static int __open_device(struct dm_pool_metadata *pmd,
 	(*td)->creation_time = le32_to_cpu(details_le.creation_time);
 	(*td)->snapshotted_time = le32_to_cpu(details_le.snapshotted_time);
 
+	dm_alloc_context_get(pmd->data_extents, &(*td)->data_alloc);
 	list_add(&(*td)->list, &pmd->thin_devices);
 
 	return 0;
@@ -1263,7 +1295,8 @@ static int __delete_device(struct dm_pool_metadata *pmd, dm_thin_id dev)
 	}
 
 	list_del(&td->list);
-	kfree(td);
+	__free_device(td);
+
 	r = dm_btree_remove(&pmd->details_info, pmd->details_root,
 			    &key, &pmd->details_root);
 	if (r)
@@ -1849,13 +1882,29 @@ bool dm_thin_aborted_changes(struct dm_thin_device *td)
 	return r;
 }
 
-int dm_pool_alloc_data_block(struct dm_pool_metadata *pmd, dm_block_t *result)
+static int sm_alloc_extent(void *context, uint64_t b, uint64_t e, uint64_t *result)
+{
+	struct dm_pool_metadata *pmd = context;
+	return dm_sm_new_block_in_range(pmd->data_sm, b, e, result);
+}
+
+int dm_thin_alloc_data_block(struct dm_thin_device *td, dm_block_t *result)
 {
 	int r = -EINVAL;
+	struct dm_pool_metadata *pmd = td->pmd;
 
 	pmd_write_lock(pmd);
-	if (!pmd->fail_io)
-		r = dm_sm_new_block(pmd->data_sm, result);
+	if (!pmd->fail_io) {
+		r = dm_alloc_context_alloc(&td->data_alloc, sm_alloc_extent, pmd, result);
+		if (r == -ENOSPC) {
+			/*
+			 * If we've run out of space we retry in case any block have been
+			 * freed since we last resized/reset the extent allocator.
+			 */
+			dm_extent_allocator_reset(pmd->data_extents);
+			r = dm_alloc_context_alloc(&td->data_alloc, sm_alloc_extent, pmd, result);
+		}
+	}
 	pmd_write_unlock(pmd);
 
 	return r;
@@ -2050,8 +2099,11 @@ int dm_pool_resize_data_dev(struct dm_pool_metadata *pmd, dm_block_t new_count)
 	int r = -EINVAL;
 
 	pmd_write_lock(pmd);
-	if (!pmd->fail_io)
+	if (!pmd->fail_io) {
 		r = __resize_space_map(pmd->data_sm, new_count);
+		if (!r)
+			dm_extent_allocator_resize(pmd->data_extents, new_count);
+	}
 	pmd_write_unlock(pmd);
 
 	return r;
diff --git a/drivers/md/dm-thin-metadata.h b/drivers/md/dm-thin-metadata.h
index 2f64f48b5f19..1003b9c7f2d7 100644
--- a/drivers/md/dm-thin-metadata.h
+++ b/drivers/md/dm-thin-metadata.h
@@ -159,7 +159,7 @@ int dm_thin_find_mapped_range(struct dm_thin_device *td,
 /*
  * Obtain an unused block.
  */
-int dm_pool_alloc_data_block(struct dm_pool_metadata *pmd, dm_block_t *result);
+int dm_thin_alloc_data_block(struct dm_thin_device *td, dm_block_t *result);
 
 /*
  * Insert or remove block.
diff --git a/drivers/md/dm-thin.c b/drivers/md/dm-thin.c
index 07c7f9795b10..518ba35b8819 100644
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@ -1552,12 +1552,12 @@ static int alloc_data_block(struct thin_c *tc, dm_block_t *result)
 		}
 	}
 
-	r = dm_pool_alloc_data_block(pool->pmd, result);
+	r = dm_thin_alloc_data_block(tc->td, result);
 	if (r) {
 		if (r == -ENOSPC)
 			set_pool_mode(pool, PM_OUT_OF_DATA_SPACE);
 		else
-			metadata_operation_failed(pool, "dm_pool_alloc_data_block", r);
+			metadata_operation_failed(pool, "dm_thin_alloc_data_block", r);
 		return r;
 	}
 
diff --git a/drivers/md/persistent-data/Makefile b/drivers/md/persistent-data/Makefile
index 66be7c66479a..62d2d2351502 100644
--- a/drivers/md/persistent-data/Makefile
+++ b/drivers/md/persistent-data/Makefile
@@ -4,6 +4,7 @@ dm-persistent-data-objs := \
 	dm-array.o \
 	dm-bitset.o \
 	dm-block-manager.o \
+	dm-extent-allocator.o \
 	dm-space-map-common.o \
 	dm-space-map-disk.o \
 	dm-space-map-metadata.o \
diff --git a/drivers/md/persistent-data/dm-btree.c b/drivers/md/persistent-data/dm-btree.c
index 0c7a2e8d1846..8e93c9c56608 100644
--- a/drivers/md/persistent-data/dm-btree.c
+++ b/drivers/md/persistent-data/dm-btree.c
@@ -579,7 +579,7 @@ static void redistribute2(struct btree_node *left, struct btree_node *right)
  * Redistribute entries between three nodes.  Assumes the central
  * node is empty.
  */
-static void redistribute3(struct btree_node *left, struct btree_node *center,
+extern void redistribute3(struct btree_node *left, struct btree_node *center,
 			  struct btree_node *right)
 {
 	unsigned int nr_left = le32_to_cpu(left->header.nr_entries);
diff --git a/drivers/md/persistent-data/dm-extent-allocator.c b/drivers/md/persistent-data/dm-extent-allocator.c
new file mode 100644
index 000000000000..677398b7728e
--- /dev/null
+++ b/drivers/md/persistent-data/dm-extent-allocator.c
@@ -0,0 +1,641 @@
+#include "dm-extent-allocator.h"
+
+
+/*
+ * 'extents' are just ranges of blocks that we'd like to allocate from.  These
+ * extents are held as the leaf nodes of a BSP tree.  An 'allocation context' points
+ * to a particular leaf/extent in the tree.  Multiple allocation contexts can point
+ * to the same leaf.
+ *
+ * If a leaf is exhausted, we update all allocation contexts that point to it and delete
+ * it from the tree.
+ */
+
+struct internal {
+	/*
+	 * An approximation to the number of free blocks below this internal node.
+	 * Used to guide the walk to a new leaf.
+	 */
+	uint64_t nr_free;
+
+	/* child nodes */
+	struct node *left;
+	struct node *right;
+};
+
+struct leaf {
+	/* contains allocation contexts */
+	struct list_head holders;
+};
+
+struct node {
+	/*
+	 * We sometimes walk up the tree from a leaf to a root, it's useful
+	 * to know if we were the left or right child of the parent.
+	 */
+	bool is_left_child;
+
+	/* discriminant for the union */
+	bool is_leaf;
+
+	/* nr of allocation contexts using leaves below this node */
+	uint32_t nr_holders;
+
+	/* range of blocks covered by this node */
+	uint64_t begin;
+	uint64_t end;
+
+	struct node *parent;
+
+	union {
+		struct internal internal;
+		struct leaf leaf;
+	} u;
+};
+
+/*
+ * We preallocate this many nodes in case we can't add any when setting up
+ * allocation contexts.
+ */
+#define INITIAL_NR_NODES 32
+
+/*
+ * Leaf nodes with fewer than this many free blocks cannot be split.
+ */
+#define SPLIT_THRESHOLD 16
+
+struct dm_extent_allocator {
+	struct mutex lock;
+
+	unsigned nr_preallocated_nodes;
+	unsigned nr_free_nodes;
+	struct list_head free_nodes;
+
+	unsigned nr_allocation_contexts;
+	struct list_head allocation_contexts;
+
+	uint64_t nr_blocks;
+	struct node *root;
+};
+
+static void lock(struct dm_extent_allocator *ea)
+{
+	mutex_lock(&ea->lock);
+}
+
+static void unlock(struct dm_extent_allocator *ea)
+{
+	mutex_unlock(&ea->lock);
+}
+
+/**
+ * __free_node - Frees a node in the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ * @node: Node to free.
+ */
+static inline void __free_node(struct dm_extent_allocator *ea, struct node *n)
+{
+	struct list_head *l = (struct list_head *)n;
+	list_add(l, &ea->free_nodes);
+	ea->nr_free_nodes++;
+}
+
+/**
+ * __alloc_node - Allocates a node from the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ */
+static inline struct node *__alloc_node(struct dm_extent_allocator *ea)
+{
+	struct list_head *l;
+
+	l = ea->free_nodes.next;
+	list_del(l);
+	ea->nr_free_nodes--;
+
+	return (struct node *) l;
+}
+
+/**
+ * @ea: Pointer to the extent allocator.
+ * @node: Index of the node to query.
+ *
+ * Returns: The number of free blocks in the node.
+ */
+static inline uint64_t __nr_free_blocks(struct node *n)
+{
+	if (!n)
+		return 0;
+
+	if (n->is_leaf)
+		return n->end - n->begin;
+	else
+		return n->u.internal.nr_free;
+}
+
+/**
+ * __free_tree - Frees all nodes in a tree.
+ * @ea: Pointer to the extent allocator.
+ * @n: Pointer to the root node of the tree to free.
+ */
+static void __free_tree(struct dm_extent_allocator *ea, struct node *n)
+{
+	if (!n)
+		return;
+
+	if (n->is_leaf)
+		__free_node(ea, n);
+	else {
+		__free_tree(ea, n->u.internal.left);
+		__free_tree(ea, n->u.internal.right);
+		__free_node(ea, n);
+	}
+}
+
+/**
+ * __setup_initial_root - Sets up the initial root node for the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ *
+ * The root node is a leaf node that spans the entire device.
+ */
+static void __setup_initial_root(struct dm_extent_allocator *ea)
+{
+	struct node *n;
+	struct leaf *l;
+
+	n = ea->root = __alloc_node(ea);
+	n->is_left_child = true;
+	n->is_leaf = true;
+	n->nr_holders = 0;
+	n->begin = 0;
+	n->end = ea->nr_blocks;
+	n->parent = NULL;
+
+	l = &n->u.leaf;
+	INIT_LIST_HEAD(&l->holders);
+}
+
+/**
+ * free_node_list - Frees a list of nodes.
+ * @l: Pointer to the list head of the nodes to free.
+ */
+static void free_node_list(struct list_head *l)
+{
+	struct list_head *e, *tmp;
+
+	list_for_each_safe(e, tmp, l) {
+		list_del(e);
+		kfree(e);
+	}
+}
+
+/**
+ * alloc_node_list - Allocates a list of nodes.
+ * @nr: Number of nodes to allocate.
+ * @flags: Flags to pass to kmalloc.
+ * @result: Pointer to the list head to store the allocated nodes.
+ *
+ * Used to initialise the free list of nodes.
+ * Returns: 0 on success, or -ENOMEM if allocation failed.
+ */
+static int alloc_node_list(unsigned nr, int flags, struct list_head *result)
+{
+	int i;
+
+	INIT_LIST_HEAD(result);
+
+	for (i = 0; i < nr; i++) {
+		struct node *n = kmalloc(sizeof(*n), flags);
+		struct list_head *l = (struct list_head *) n;
+		if (!n) {
+			free_node_list(result);
+			return -ENOMEM;
+		}
+
+		list_add(l, result);
+	}
+
+	return 0;
+}
+
+/**
+ * __prealloc_nodes - Preallocates nodes for allocation contexts.
+ * @ea: Pointer to the extent allocator.
+ * @nr: Number of nodes to preallocate.
+ */
+static void __prealloc_nodes(struct dm_extent_allocator *ea, unsigned nr, int flags)
+{
+	int r;
+	struct list_head new_nodes;
+
+	r = alloc_node_list(nr, flags, &new_nodes);
+	if (!r) {
+		struct list_head *e, *tmp;
+		list_for_each_safe(e, tmp, &new_nodes) {
+			list_del(e);
+			__free_node(ea, (struct node *)e);
+		}
+		ea->nr_preallocated_nodes += nr;
+	}
+}
+
+struct dm_extent_allocator *dm_extent_allocator_create(uint64_t nr_blocks)
+{
+	struct dm_extent_allocator *ea = kmalloc(sizeof(*ea), GFP_KERNEL);
+
+	if (!ea)
+		return NULL;
+
+	mutex_init(&ea->lock);
+	ea->nr_blocks = nr_blocks;
+	ea->nr_preallocated_nodes = 0;
+	ea->nr_free_nodes = 0;
+	ea->nr_allocation_contexts = 0;
+
+	INIT_LIST_HEAD(&ea->free_nodes);
+	__prealloc_nodes(ea, INITIAL_NR_NODES, GFP_KERNEL);
+	INIT_LIST_HEAD(&ea->allocation_contexts);
+	__setup_initial_root(ea);
+
+	return ea;
+}
+EXPORT_SYMBOL_GPL(dm_extent_allocator_create);
+
+void dm_extent_allocator_destroy(struct dm_extent_allocator *ea)
+{
+	__free_tree(ea, ea->root);
+	free_node_list(&ea->free_nodes);
+	kfree(ea);
+}
+EXPORT_SYMBOL_GPL(dm_extent_allocator_destroy);
+
+/**
+ * __reset - Resets the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ * @nr_blocks: New number of blocks in the device in case of resize.
+ *
+ * Frees all nodes in the tree and sets up a new root node that spans the entire device.
+ */
+static void __reset(struct dm_extent_allocator *ea, uint64_t nr_blocks)
+{
+	struct dm_alloc_context *ac, *tmp;
+
+	list_for_each_entry_safe(ac, tmp, &ea->allocation_contexts, list) {
+		if (ac->leaf)
+			list_del(&ac->holders_list);
+		ac->leaf = NULL;
+	}
+
+	__free_tree(ea, ea->root);
+
+	ea->nr_blocks = nr_blocks;
+	__setup_initial_root(ea);
+}
+
+void dm_extent_allocator_reset(struct dm_extent_allocator *ea)
+{
+	lock(ea);
+	__reset(ea, ea->nr_blocks);
+	unlock(ea);
+}
+EXPORT_SYMBOL_GPL(dm_extent_allocator_reset);
+
+void dm_extent_allocator_resize(struct dm_extent_allocator *ea,
+				uint64_t nr_blocks)
+{
+	lock(ea);
+	__reset(ea, nr_blocks);
+	unlock(ea);
+}
+EXPORT_SYMBOL_GPL(dm_extent_allocator_resize);
+
+/**
+ * __split_leaf - Splits a leaf node in the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ * @node: leaf node to split.
+ *
+ * The split point is chosen to be the midpoint of the leaf's range. The new leaf
+ * node is added as the right child of a new internal node, which is added as the
+ * parent of the original leaf node. The function returns the index of the new
+ * internal node on success, or NULL on failure.
+ */
+static struct node *__split_leaf(struct dm_extent_allocator *ea, struct node *n)
+{
+	uint64_t mid;
+	struct node *new_parent, *right_child;
+
+	if (ea->nr_free_nodes < 2)
+		return NULL;
+
+	if (n->end - n->begin <= SPLIT_THRESHOLD)
+		return NULL;
+
+	new_parent = __alloc_node(ea);
+	right_child = __alloc_node(ea);
+
+	mid = n->begin + (n->end - n->begin) / 2;
+
+	new_parent->is_left_child = false;
+	right_child->is_leaf = true;
+	right_child->nr_holders = 0;
+	right_child->begin = mid;
+	right_child->end = n->end;
+	right_child->parent = new_parent;
+	INIT_LIST_HEAD(&right_child->u.leaf.holders);
+
+	new_parent->is_left_child = n->is_left_child;
+	new_parent->is_leaf = false;
+	new_parent->nr_holders = n->nr_holders + 1;
+	new_parent->begin = n->begin;
+	new_parent->end = n->end;
+	new_parent->parent = n->parent;
+	new_parent->u.internal.nr_free = n->end - n->begin;
+	new_parent->u.internal.left = n;
+	new_parent->u.internal.right = right_child;
+
+	/* the original leaf becomes the left child */
+	n->is_left_child = true;
+	n->end = mid;
+	n->parent = new_parent;
+
+	return new_parent;
+}
+
+/**
+ * __select_child - Selects the best child node to allocate from in the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ * @left: left child node.
+ * @right: right child node.
+ *
+ * The best child is the one with the highest ratio of free blocks to holders. If the
+ * ratios are equal, the left child is preferred.
+ */
+static struct node **__select_child(struct dm_extent_allocator *ea, struct node **left, struct node **right)
+{
+	uint64_t left_score, right_score;
+
+	left_score = __nr_free_blocks(*left) / ((*left)->nr_holders + 1);
+	right_score = __nr_free_blocks(*right) / ((*right)->nr_holders + 1);
+
+	if (left_score >= right_score)
+		return left;
+	else
+		return right;
+}
+
+/**
+ * __get_leaf - Gets a leaf node from the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ *
+ * The function walks the tree from the root to a leaf, selecting the best child node at each
+ * step. If the selected child node is already in use, the function will attempt to split the
+ * node with __split_leaf.  If splitting fails the leaf node will be shared.
+ *
+ * Returns: A pointer to the leaf node on success, or NULL if there are no more free blocks.
+ */
+static struct node *__get_leaf(struct dm_extent_allocator *ea)
+{
+	struct node **ptr = &ea->root;
+	struct node *n = *ptr;
+
+	if (!ea->root)
+		return NULL;
+
+	/* walk the tree until we get to a leaf */
+	while (!n->is_leaf) {
+		struct internal *i = &n->u.internal;
+		n->nr_holders++;
+		ptr = __select_child(ea, &i->left, &i->right);
+		n = *ptr;
+	}
+
+	if (n->nr_holders > 0) {
+		/*
+		* Someone is already using this extent.  See if we can split it.
+		*/
+		struct node *split = __split_leaf(ea, n);
+		if (split) {
+			/* patch up the parent */
+			*ptr = split;
+
+			/* the new leaf is the right child */
+			n = split->u.internal.right;
+		}
+	}
+
+	n->nr_holders++;
+	return n;
+}
+
+/**
+ * __put_leaf - Releases a leaf node and updates the extent allocator's tree.
+ * @ea: Pointer to the extent allocator.
+ * @n: Pointer to the leaf node to release.
+ * @delta: Number of holders to release.
+ */
+static void __put_leaf(struct dm_extent_allocator *ea, struct node *n, uint32_t delta)
+{
+	bool empty;
+	struct node *parent, *grand_parent;
+
+	if (!n)
+		return;
+
+	parent = n->parent;
+
+	/* adjust leaf */
+	n->nr_holders -= delta;
+
+	/* see if the leaf is now empty */
+	empty = n->begin == n->end;
+	if (empty && !n->nr_holders) {
+		bool n_is_left = n->is_left_child;
+		__free_node(ea, n);
+
+		if (parent) {
+			bool is_left = parent->is_left_child;
+
+			/*
+			* We also free the parent, since every internal node
+			* must have two children.
+			*/
+			grand_parent = parent->parent;
+
+			/* replace the parent with the other child */
+			if (n_is_left)
+				n = parent->u.internal.right;
+			else
+				n = parent->u.internal.left;
+
+			__free_node(ea, parent);
+			parent = grand_parent;
+			n->parent = parent;
+
+			if (parent) {
+				/* patch up the parent */
+				if (is_left) {
+					n->is_left_child = true;
+					parent->u.internal.left = n;
+				} else {
+					n->is_left_child = false;
+					parent->u.internal.right = n;
+				}
+			}
+		} else
+			n = NULL;
+	}
+
+	/* walk up the tree adjusting the counts */
+	while (parent) {
+		parent->nr_holders -= delta;
+		parent->u.internal.nr_free =
+			__nr_free_blocks(parent->u.internal.left) +
+			__nr_free_blocks(parent->u.internal.right);
+
+		n = parent;
+		parent = n->parent;
+	}
+
+	ea->root = n;
+}
+
+/**
+ * __next_leaf - Gets the next available leaf node for an allocation context.
+ * @ac: Pointer to the allocation context.
+ *
+ * Returns: 0 on success, or -ENOSPC if there are no more free blocks.
+ */
+static int __next_leaf(struct dm_alloc_context *ac)
+{
+	struct node *l;
+	struct dm_extent_allocator *ea = ac->ea;
+
+	ac->leaf = __get_leaf(ea);
+	if (!ac->leaf)
+		return -ENOSPC;
+
+	l = ac->leaf;
+	list_add(&ac->holders_list, &l->u.leaf.holders);
+	return 0;
+}
+
+void dm_alloc_context_get(struct dm_extent_allocator *ea,
+			  struct dm_alloc_context *ac)
+{
+	lock(ea);
+	ac->ea = ea;
+
+	ea->nr_allocation_contexts++;
+
+	/*
+	 * We try and maintain a couple of nodes per alloc context to avoid sharing.
+	 * If allocation fails it's no big deal; we'll just get more fragmentation.
+	 */ 
+	if (ea->nr_preallocated_nodes < ea->nr_allocation_contexts * 2)
+		__prealloc_nodes(ea, 2, GFP_NOIO);
+
+	list_add(&ac->list, &ea->allocation_contexts);
+	INIT_LIST_HEAD(&ac->holders_list);
+	ac->leaf = NULL;
+	unlock(ea);
+}
+EXPORT_SYMBOL_GPL(dm_alloc_context_get);
+
+void dm_alloc_context_put(struct dm_alloc_context *ac)
+{
+	struct dm_extent_allocator *ea = ac->ea;
+
+	lock(ea);
+	if (ac->leaf)
+		list_del(&ac->holders_list);
+	list_del(&ac->list);
+	__put_leaf(ea, ac->leaf, 1);
+	ea->nr_allocation_contexts--;
+	unlock(ea);
+}
+EXPORT_SYMBOL_GPL(dm_alloc_context_put);
+
+/**
+ * __reset_contexts - Resets all allocation contexts that are currently using a leaf node.
+ * @ac: Pointer to the allocation context.
+ */
+static void __reset_contexts(struct dm_alloc_context *ac)
+{
+	struct node *n = ac->leaf;
+	struct leaf *l = &n->u.leaf;
+	struct dm_alloc_context *ac_it, *tmp;
+
+	list_for_each_entry_safe(ac_it, tmp, &l->holders, holders_list) {
+		ac_it->leaf = NULL;
+		list_del(&ac_it->holders_list);
+	}
+}
+
+/**
+ * __reset_and_release - Resets all allocation contexts that are currently using a leaf node
+ *                       then releases the leaf node.
+ * @ac: Pointer to the allocation context.
+ */
+static void __reset_and_release(struct dm_alloc_context *ac)
+{
+	struct node *old_leaf = ac->leaf;
+
+	__reset_contexts(ac); /* this clobbers ac->leaf */
+	__put_leaf(ac->ea, old_leaf, old_leaf->nr_holders);
+}
+
+
+static int __alloc(struct dm_alloc_context *ac, dm_alloc_extent_fn fn,
+	    void *context, uint64_t *result)
+{
+	int r = 0;
+	struct node *n;
+
+	while (true) {
+		/* do we have a leaf? */
+		if (!ac->leaf) {
+			r = __next_leaf(ac);
+			if (r)
+				return r;
+		}
+
+		n = ac->leaf;
+
+		/* does the leaf have space? */
+		if (n->begin == n->end) {
+			__reset_and_release(ac);
+			continue;
+		}
+
+		/* call down to the underlying allocator */
+		r = fn(context, n->begin, n->end, result);
+		if (r == -ENOSPC) {
+			n->begin = n->end;
+			__reset_and_release(ac);
+			continue;
+		}
+
+		if (!r) {
+			/* success */
+			n->begin = *result + 1;
+
+			if (n->begin == n->end)
+				__reset_and_release(ac);
+		}
+
+		return r;
+	}
+}
+
+int dm_alloc_context_alloc(struct dm_alloc_context *ac, dm_alloc_extent_fn fn,
+			   void *context, uint64_t *result)
+{
+	int r;
+	struct dm_extent_allocator *ea = ac->ea;
+
+	lock(ea);
+	r = __alloc(ac, fn, context, result);
+	unlock(ea);
+
+	return r;
+}
+EXPORT_SYMBOL_GPL(dm_alloc_context_alloc);
diff --git a/drivers/md/persistent-data/dm-extent-allocator.h b/drivers/md/persistent-data/dm-extent-allocator.h
new file mode 100644
index 000000000000..c24e6b03ea82
--- /dev/null
+++ b/drivers/md/persistent-data/dm-extent-allocator.h
@@ -0,0 +1,106 @@
+/*
+ * Copyright (C) 2023 Red Hat, Inc.
+ *
+ * This file is released under the GPL.
+ */
+
+#ifndef _LINUX_DM_EXTENT_ALLOCATOR_H
+#define _LINUX_DM_EXTENT_ALLOCATOR_H
+
+#include "dm-block-manager.h"
+
+/*
+ * This extent allocator is used to supervise allocations of data blocks for
+ * thin provisioning.  It enhances data locality and reduces fragmentation by
+ * allocating contiguous extents of blocks.
+ *
+ * An extent allocator instance is created with a fixed number of blocks. An
+ * 'allocation context' abstraction is provided to manage contiguous allocations.
+ * Each allocation context aims to allocate as few linear ranges of blocks as
+ * possible, keeping different contexts isolated from each other.
+ *
+ * There are three categories of operations supported:
+ * 1. Allocator-wide operations: create, destroy and reset.
+ * 2. Context-related operations: getting and putting allocation contexts.
+ * 3. Block allocation within a context: allocation of a new block.
+ *
+ * All methods provided in this interface except create/destroy are thread-safe.
+ */
+
+struct dm_extent_allocator;
+
+// Treat this structure as opaque
+struct dm_alloc_context {
+	struct dm_extent_allocator *ea;
+
+	struct list_head list;
+	struct list_head holders_list;
+	void *leaf;
+};
+
+/**
+ * dm_extent_allocator_create - Creates a new extent allocator.
+ * @nr_blocks: Number of blocks in the device.
+ *
+ * The allocator is initialized with a root node that spans the entire device and
+ * has no holders. The function returns a pointer to the new extent allocator on
+ * success, or NULL on failure.
+ */
+struct dm_extent_allocator *
+dm_extent_allocator_create(uint64_t nr_blocks);
+
+/**
+ * dm_extent_allocator_destroy - Destroys an extent allocator.
+ * @ea: Pointer to the extent allocator.
+ *
+ * Assumes that there are no active allocation contexts.
+ */
+void dm_extent_allocator_destroy(struct dm_extent_allocator *ea);
+
+/**
+ * dm_extent_allocator_reset - Resets an extent allocator to its initial state.
+ * @ea: Pointer to the extent allocator.
+ *
+ * Resets an extent allocator to its initial state by freeing all of its nodes
+ * and resetting allocation contexts, it then sets up a new root node that spans the
+ * entire extent and has no holders.
+ */
+void dm_extent_allocator_reset(struct dm_extent_allocator *ea);
+
+/**
+ * dm_extent_allocator_resize - Resizes an extent allocator to a new size.
+ * @ea: Pointer to the extent allocator.
+ * @nr_blocks: New number of blocks in the device.
+ */
+void dm_extent_allocator_resize(struct dm_extent_allocator *ea,
+				uint64_t nr_blocks);
+
+/**
+ * dm_alloc_context_get - Gets a new allocation context for the extent allocator.
+ * @ea: Pointer to the extent allocator.
+ * @ac: Pointer to the allocation context to initialize.
+ */
+void dm_alloc_context_get(struct dm_extent_allocator *ea, struct dm_alloc_context *ac);
+
+/**
+ * dm_alloc_context_put - Releases an allocation context for the extent allocator.
+ * @ac: Pointer to the allocation context to release.
+ */
+void dm_alloc_context_put(struct dm_alloc_context *ac);
+
+/**
+ * dm_alloc_context_alloc - Allocates a new block in the extent allocator.
+ * @ac: Pointer to the allocation context.
+ * @result: Pointer to a variable to store the allocated block number.
+ *
+ * A callback is used for the fine grain allocation decision (eg, using a space map).
+ * The function returns 0 on success, or -ENOSPC if there are no more free blocks.
+ * The allocated block number is stored in the variable pointed to by @result.
+ */
+typedef int (*dm_alloc_extent_fn)(void *context, uint64_t b, uint64_t e,
+				  uint64_t *result);
+int dm_alloc_context_alloc(struct dm_alloc_context *ac,
+			 dm_alloc_extent_fn fn, void *context,
+			 uint64_t *result);
+
+#endif
\ No newline at end of file
diff --git a/drivers/md/persistent-data/dm-space-map-common.c b/drivers/md/persistent-data/dm-space-map-common.c
index 591d1a43d035..97de3665da8e 100644
--- a/drivers/md/persistent-data/dm-space-map-common.c
+++ b/drivers/md/persistent-data/dm-space-map-common.c
@@ -397,7 +397,7 @@ int sm_ll_find_common_free_block(struct ll_disk *old_ll, struct ll_disk *new_ll,
 	uint32_t count;
 
 	do {
-		r = sm_ll_find_free_block(new_ll, begin, new_ll->nr_blocks, b);
+		r = sm_ll_find_free_block(new_ll, begin, end, b);
 		if (r)
 			break;
 
diff --git a/drivers/md/persistent-data/dm-space-map-disk.c b/drivers/md/persistent-data/dm-space-map-disk.c
index f4241f54e20e..c0661184c672 100644
--- a/drivers/md/persistent-data/dm-space-map-disk.c
+++ b/drivers/md/persistent-data/dm-space-map-disk.c
@@ -155,6 +155,28 @@ static int sm_disk_new_block(struct dm_space_map *sm, dm_block_t *b)
 	return r;
 }
 
+static int sm_disk_new_block_in_range(struct dm_space_map *sm, dm_block_t b, dm_block_t e, dm_block_t *result)
+{
+	int r;
+	int32_t nr_allocations;
+	struct sm_disk *smd = container_of(sm, struct sm_disk, sm);
+
+	/*
+	 * Any block we allocate has to be free in both the old and current ll.
+	 */
+	r = sm_ll_find_common_free_block(&smd->old_ll, &smd->ll, b, e, result);
+	if (r)
+		return r;
+
+	r = sm_ll_inc(&smd->ll, *result, *result + 1, &nr_allocations);
+	if (!r)
+		smd->nr_allocated_this_transaction += nr_allocations;
+
+	return r;
+}
+
+//--------------------
+
 static int sm_disk_commit(struct dm_space_map *sm)
 {
 	int r;
@@ -208,6 +230,7 @@ static struct dm_space_map ops = {
 	.inc_blocks = sm_disk_inc_blocks,
 	.dec_blocks = sm_disk_dec_blocks,
 	.new_block = sm_disk_new_block,
+	.new_block_in_range = sm_disk_new_block_in_range,
 	.commit = sm_disk_commit,
 	.root_size = sm_disk_root_size,
 	.copy_root = sm_disk_copy_root,
diff --git a/drivers/md/persistent-data/dm-space-map.h b/drivers/md/persistent-data/dm-space-map.h
index 6bf69922b5ad..25e53ae43b80 100644
--- a/drivers/md/persistent-data/dm-space-map.h
+++ b/drivers/md/persistent-data/dm-space-map.h
@@ -54,6 +54,8 @@ struct dm_space_map {
 	 * new_block will increment the returned block.
 	 */
 	int (*new_block)(struct dm_space_map *sm, dm_block_t *b);
+	int (*new_block_in_range)(struct dm_space_map *sm, dm_block_t b, dm_block_t e,
+				  dm_block_t *result);
 
 	/*
 	 * The root contains all the information needed to fix the space map.
@@ -144,6 +146,12 @@ static inline int dm_sm_new_block(struct dm_space_map *sm, dm_block_t *b)
 	return sm->new_block(sm, b);
 }
 
+static inline int dm_sm_new_block_in_range(struct dm_space_map *sm, dm_block_t b, dm_block_t e,
+					   dm_block_t *result)
+{
+	return sm->new_block_in_range(sm, b, e, result);
+}
+
 static inline int dm_sm_root_size(struct dm_space_map *sm, size_t *result)
 {
 	return sm->root_size(sm, result);
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 8228d1276a2f..65966f93455a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -589,6 +589,7 @@ void *kmalloc_large_node(size_t size, gfp_t flags, int node) __assume_page_align
  */
 static __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)
 {
+#if 0
 	if (__builtin_constant_p(size) && size) {
 		unsigned int index;
 
@@ -600,6 +601,7 @@ static __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)
 				kmalloc_caches[kmalloc_type(flags, _RET_IP_)][index],
 				flags, size);
 	}
+#endif
 	return __kmalloc(size, flags);
 }
 
diff --git a/scripts/mod/modpost.c b/scripts/mod/modpost.c
index b3dee80497cb..f01a57392788 100644
--- a/scripts/mod/modpost.c
+++ b/scripts/mod/modpost.c
@@ -1068,21 +1068,168 @@ static inline int is_valid_name(struct elf_info *elf, Elf_Sym *sym)
 	return !is_mapping_symbol(name);
 }
 
-/* Look up the nearest symbol based on the section and the address */
-static Elf_Sym *find_nearest_sym(struct elf_info *elf, Elf_Addr addr,
-				 unsigned int secndx, bool allow_negative,
-				 Elf_Addr min_distance)
+/*-------------------------*/
+
+struct sym_section {
+	struct list_head list;
+
+	unsigned sec_index;
+	unsigned nr_symbols;
+	unsigned nr_set;
+	Elf_Sym **sym;
+};
+
+#define SECTION_HASH_SIZE 1024
+
+struct sym_table {
+	struct elf_info *elf;
+	struct list_head sections[SECTION_HASH_SIZE];
+};
+
+static unsigned hash_slot(unsigned sec_index)
+{
+	/* probably good enough */
+	return sec_index % SECTION_HASH_SIZE;
+}
+
+static struct sym_section *get_section(struct sym_table *table, unsigned sec_index)
+{
+	struct sym_section *sec;
+	struct list_head *bucket = &table->sections[hash_slot(sec_index)];
+
+	list_for_each_entry (sec, bucket, list) {
+		if (sec->sec_index == sec_index)
+			return sec;
+	}
+
+	/* new section */
+	sec = malloc(sizeof(*sec));
+	if (!sec)
+		fatal("Out of memory\n");
+
+	sec->sec_index = sec_index;
+	sec->nr_symbols = 0;
+	sec->nr_set = 0;
+	sec->sym = NULL;
+
+	list_add_tail(&sec->list, bucket);
+
+	return sec;
+}
+
+static int sym_cmp_addr(const void *a, const void *b)
+{
+	const Elf_Sym *sym_a = *(const Elf_Sym **)a;
+	const Elf_Sym *sym_b = *(const Elf_Sym **)b;
+
+	if (sym_a->st_value < sym_b->st_value)
+		return -1;
+	if (sym_a->st_value > sym_b->st_value)
+		return 1;
+	return 0;
+}
+
+// FIXME: handle is_arm
+static struct sym_table *build_sym_table(struct elf_info *elf)
+{
+	unsigned i;
+	struct list_head *bucket;
+	struct sym_table *table;
+	struct sym_section *sec;
+	Elf_Sym *sym;
+
+	table = malloc(sizeof(*table));
+	if (!table)
+		fatal("Out of memory\n");
+
+	table->elf = elf;
+
+	for (unsigned i = 0; i < SECTION_HASH_SIZE; i++)
+		INIT_LIST_HEAD(&table->sections[i]);
+
+	for (sym = elf->symtab_start; sym < elf->symtab_stop; sym++) {
+		sec = get_section(table, get_secindex(elf, sym));
+
+		if (!is_valid_name(elf, sym))
+			continue;
+
+		sec->nr_symbols++;
+	}
+
+	for (i = 0; i < SECTION_HASH_SIZE; i++) {
+		bucket = &table->sections[i];
+		list_for_each_entry (sec, bucket, list) {
+			if (sec->nr_symbols) {
+				sec->sym = malloc(sizeof(sym) * sec->nr_symbols);
+				if (!sec->sym)
+					fatal("Out of memory\n");
+			}
+		}
+	}
+
+	for (sym = elf->symtab_start; sym < elf->symtab_stop; sym++) {
+		sec = get_section(table, get_secindex(elf, sym));
+
+		if (!is_valid_name(elf, sym))
+			continue;
+
+		if (sec->nr_set >= sec->nr_symbols) {
+			fatal("sec->nr_set=%u >= sec->nr_symbols=%u\n",
+			      sec->nr_set, sec->nr_symbols);
+		}
+		sec->sym[sec->nr_set++] = sym;
+	}
+
+	for (i = 0; i < SECTION_HASH_SIZE; i++) {
+		bucket = &table->sections[i];
+		list_for_each_entry(sec, bucket, list) {
+			/* sort by sym addr */
+			qsort(sec->sym, sec->nr_symbols, sizeof(sym),
+			      sym_cmp_addr);
+		}
+	}
+
+	return table;
+}
+
+static unsigned lower_bound(struct sym_section *section, Elf_Addr addr)
+{
+	Elf_Sym *sym;
+	int lo = -1, hi = section->nr_symbols;
+
+	while (hi - lo > 1) {
+		int mid = lo + ((hi - lo) / 2);
+		sym = section->sym[mid];
+
+		if (sym->st_value == addr)
+			return mid;
+
+		if (sym->st_value < addr)
+			lo = mid;
+		else
+			hi = mid;
+	}
+
+	return lo < 0 ? 0 : lo;
+}
+
+static Elf_Sym *
+find_nearest_sym(struct sym_table *table, Elf_Addr addr, unsigned int secndx,
+		 bool allow_negative, Elf_Addr min_distance)
 {
 	Elf_Sym *sym;
-	Elf_Sym *near = NULL;
+	Elf_Sym *best_sym = NULL;
 	Elf_Addr sym_addr, distance;
-	bool is_arm = (elf->hdr->e_machine == EM_ARM);
+	bool is_arm = (table->elf->hdr->e_machine == EM_ARM);
 
-	for (sym = elf->symtab_start; sym < elf->symtab_stop; sym++) {
-		if (get_secindex(elf, sym) != secndx)
-			continue;
-		if (!is_valid_name(elf, sym))
-			continue;
+	struct sym_section *sec = get_section(table, secndx);
+	unsigned int sym_index = lower_bound(sec, addr);
+
+	unsigned loop_count = 0;
+	for (sym_index = lower_bound(sec, addr); sym_index < sec->nr_symbols;
+	     sym_index++) {
+		loop_count++;
+		sym = sec->sym[sym_index];
 
 		sym_addr = sym->st_value;
 
@@ -1091,43 +1238,46 @@ static Elf_Sym *find_nearest_sym(struct elf_info *elf, Elf_Addr addr,
 		 * if the symbol is STT_FUNC type. Mask it to get the address.
 		 */
 		if (is_arm && ELF_ST_TYPE(sym->st_info) == STT_FUNC)
-			 sym_addr &= ~1;
+			sym_addr &= ~1;
 
 		if (addr >= sym_addr)
 			distance = addr - sym_addr;
-		else if (allow_negative)
+		else if (allow_negative) {
 			distance = sym_addr - addr;
-		else
-			continue;
+			if (distance > min_distance)
+				break;
+		} else
+			break;
 
 		if (distance <= min_distance) {
 			min_distance = distance;
-			near = sym;
+			best_sym = sym;
 		}
 
 		if (min_distance == 0)
 			break;
 	}
-	return near;
+
+	return best_sym;
 }
 
-static Elf_Sym *find_fromsym(struct elf_info *elf, Elf_Addr addr,
+static Elf_Sym *find_fromsym(struct sym_table *table, Elf_Addr addr,
 			     unsigned int secndx)
 {
-	return find_nearest_sym(elf, addr, secndx, false, ~0);
+	return find_nearest_sym(table, addr, secndx, false, ~0);
 }
 
-static Elf_Sym *find_tosym(struct elf_info *elf, Elf_Addr addr, Elf_Sym *sym)
+static Elf_Sym *find_tosym(struct sym_table *table, Elf_Addr addr, Elf_Sym *sym)
 {
 	/* If the supplied symbol has a valid name, return it */
-	if (is_valid_name(elf, sym))
+	if (is_valid_name(table->elf, sym))
 		return sym;
 
 	/*
 	 * Strive to find a better symbol name, but the resulting name may not
 	 * match the symbol referenced in the original code.
 	 */
-	return find_nearest_sym(elf, addr, get_secindex(elf, sym), true, 20);
+	return find_nearest_sym(table, addr, get_secindex(table->elf, sym), true, 20);
 }
 
 static bool is_executable_section(struct elf_info *elf, unsigned int secndx)
@@ -1138,7 +1288,7 @@ static bool is_executable_section(struct elf_info *elf, unsigned int secndx)
 	return (elf->sechdrs[secndx].sh_flags & SHF_EXECINSTR) != 0;
 }
 
-static void default_mismatch_handler(const char *modname, struct elf_info *elf,
+static void default_mismatch_handler(const char *modname, struct sym_table *table,
 				     const struct sectioncheck* const mismatch,
 				     Elf_Sym *tsym,
 				     unsigned int fsecndx, const char *fromsec, Elf_Addr faddr,
@@ -1148,11 +1298,11 @@ static void default_mismatch_handler(const char *modname, struct elf_info *elf,
 	const char *tosym;
 	const char *fromsym;
 
-	from = find_fromsym(elf, faddr, fsecndx);
-	fromsym = sym_name(elf, from);
+	from = find_fromsym(table, faddr, fsecndx);
+	fromsym = sym_name(table->elf, from);
 
-	tsym = find_tosym(elf, taddr, tsym);
-	tosym = sym_name(elf, tsym);
+	tsym = find_tosym(table, taddr, tsym);
+	tosym = sym_name(table->elf, tsym);
 
 	/* check whitelist - we may ignore it */
 	if (!secref_whitelist(fromsec, fromsym, tosec, tosym))
@@ -1171,7 +1321,7 @@ static void default_mismatch_handler(const char *modname, struct elf_info *elf,
 			      "You might get more information about where this is\n"
 			      "coming from by using scripts/check_extable.sh %s\n",
 			      fromsec, (long)faddr, tosec, modname);
-		else if (is_executable_section(elf, get_secindex(elf, tsym)))
+		else if (is_executable_section(table->elf, get_secindex(table->elf, tsym)))
 			warn("The relocation at %s+0x%lx references\n"
 			     "section \"%s\" which is not in the list of\n"
 			     "authorized sections.  If you're adding a new section\n"
@@ -1186,7 +1336,7 @@ static void default_mismatch_handler(const char *modname, struct elf_info *elf,
 	}
 }
 
-static void check_export_symbol(struct module *mod, struct elf_info *elf,
+static void check_export_symbol(struct module *mod, struct sym_table *table,
 				Elf_Addr faddr, const char *secname,
 				Elf_Sym *sym)
 {
@@ -1195,9 +1345,10 @@ static void check_export_symbol(struct module *mod, struct elf_info *elf,
 	Elf_Sym *label;
 	struct symbol *s;
 	bool is_gpl;
+	struct elf_info *elf = table->elf;
 
-	label = find_fromsym(elf, faddr, elf->export_symbol_secndx);
-	label_name = sym_name(elf, label);
+	label = find_fromsym(table, faddr, elf->export_symbol_secndx);
+	label_name = sym_name(table->elf, label);
 
 	if (!strstarts(label_name, prefix)) {
 		error("%s: .export_symbol section contains strange symbol '%s'\n",
@@ -1256,16 +1407,16 @@ static void check_export_symbol(struct module *mod, struct elf_info *elf,
 		     mod->name, name);
 }
 
-static void check_section_mismatch(struct module *mod, struct elf_info *elf,
+static void check_section_mismatch(struct module *mod, struct sym_table *table,
 				   Elf_Sym *sym,
 				   unsigned int fsecndx, const char *fromsec,
 				   Elf_Addr faddr, Elf_Addr taddr)
 {
-	const char *tosec = sec_name(elf, get_secindex(elf, sym));
+	const char *tosec = sec_name(table->elf, get_secindex(table->elf, sym));
 	const struct sectioncheck *mismatch;
 
-	if (module_enabled && elf->export_symbol_secndx == fsecndx) {
-		check_export_symbol(mod, elf, faddr, tosec, sym);
+	if (module_enabled && table->elf->export_symbol_secndx == fsecndx) {
+		check_export_symbol(mod, table, faddr, tosec, sym);
 		return;
 	}
 
@@ -1273,7 +1424,7 @@ static void check_section_mismatch(struct module *mod, struct elf_info *elf,
 	if (!mismatch)
 		return;
 
-	default_mismatch_handler(mod->name, elf, mismatch, sym,
+	default_mismatch_handler(mod->name, table, mismatch, sym,
 				 fsecndx, fromsec, faddr,
 				 tosec, taddr);
 }
@@ -1482,9 +1633,10 @@ static void get_rel_type_and_sym(struct elf_info *elf, uint64_t r_info,
 	*r_sym = ELF_R_SYM(r_info);
 }
 
-static void section_rela(struct module *mod, struct elf_info *elf,
+static void section_rela(struct module *mod, struct sym_table *table,
 			 Elf_Shdr *sechdr)
 {
+	struct elf_info *elf = table->elf;
 	Elf_Rela *rela;
 	unsigned int fsecndx = sechdr->sh_info;
 	const char *fromsec = sec_name(elf, fsecndx);
@@ -1517,14 +1669,15 @@ static void section_rela(struct module *mod, struct elf_info *elf,
 			break;
 		}
 
-		check_section_mismatch(mod, elf, elf->symtab_start + r_sym,
+		check_section_mismatch(mod, table, elf->symtab_start + r_sym,
 				       fsecndx, fromsec, r_offset, taddr);
 	}
 }
 
-static void section_rel(struct module *mod, struct elf_info *elf,
+static void section_rel(struct module *mod, struct sym_table *table,
 			Elf_Shdr *sechdr)
 {
+	struct elf_info *elf = table->elf;
 	Elf_Rel *rel;
 	unsigned int fsecndx = sechdr->sh_info;
 	const char *fromsec = sec_name(elf, fsecndx);
@@ -1561,7 +1714,7 @@ static void section_rel(struct module *mod, struct elf_info *elf,
 			fatal("Please add code to calculate addend for this architecture\n");
 		}
 
-		check_section_mismatch(mod, elf, tsym,
+		check_section_mismatch(mod, table, tsym,
 				       fsecndx, fromsec, r_offset, taddr);
 	}
 }
@@ -1578,9 +1731,10 @@ static void section_rel(struct module *mod, struct elf_info *elf,
  * to find all references to a section that reference a section that will
  * be discarded and warns about it.
  **/
-static void check_sec_ref(struct module *mod, struct elf_info *elf)
+static void check_sec_ref(struct module *mod, struct sym_table *table)
 {
 	int i;
+	struct elf_info *elf = table->elf;
 	Elf_Shdr *sechdrs = elf->sechdrs;
 
 	/* Walk through all sections */
@@ -1588,9 +1742,9 @@ static void check_sec_ref(struct module *mod, struct elf_info *elf)
 		check_section(mod->name, elf, &elf->sechdrs[i]);
 		/* We want to process only relocation sections and not .init */
 		if (sechdrs[i].sh_type == SHT_RELA)
-			section_rela(mod, elf, &elf->sechdrs[i]);
+			section_rela(mod, table, &elf->sechdrs[i]);
 		else if (sechdrs[i].sh_type == SHT_REL)
-			section_rel(mod, elf, &elf->sechdrs[i]);
+			section_rel(mod, table, &elf->sechdrs[i]);
 	}
 }
 
@@ -1712,6 +1866,7 @@ static void read_symbols(const char *modname)
 	char *namespace;
 	struct module *mod;
 	struct elf_info info = { };
+	struct sym_table *table;
 	Elf_Sym *sym;
 
 	if (!parse_elf(&info, modname))
@@ -1754,7 +1909,12 @@ static void read_symbols(const char *modname)
 		handle_moddevtable(mod, &info, sym, symname);
 	}
 
-	check_sec_ref(mod, &info);
+	table = build_sym_table(&info);
+	if (!table) {
+		warn("couldn't build symbol table");
+		return;
+	}
+	check_sec_ref(mod, table);
 
 	if (!mod->is_vmlinux) {
 		version = get_modinfo(&info, "version");
diff --git a/setup-cross-compile b/setup-cross-compile
new file mode 100644
index 000000000000..9e1743eda9fc
--- /dev/null
+++ b/setup-cross-compile
@@ -0,0 +1,4 @@
+# source this file
+
+export CROSS_COMPILE=riscv64-linux-gnu-
+export ARCH=riscv
